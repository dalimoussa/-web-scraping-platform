"""
FAST Election Data Scraper - Optimized for cached data
Uses cached responses with minimal delay for maximum speed
"""

import sys
from pathlib import Path
import re
from typing import List, Dict, Any, Set
from urllib.parse import urljoin
import time
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.core.http_client import HTTPClient
from src.core.csv_exporter import CSVExporter
from src.utils.parsers import parse_html


class FastElectionScraper:
    """
    OPTIMIZED scraper that:
    1. Uses cached data aggressively (0.1s delay for cached)
    2. Only slows down for NEW requests (1s delay)
    3. Saves progress every 1000 records
    4. Can resume from interruption
    """
    
    def __init__(self, delay_cached: float = 0.1, delay_new: float = 1.0):
        """Initialize with separate delays for cached vs new requests."""
        # Use cache aggressively
        self.http = HTTPClient(
            default_delay=delay_cached,  # Fast for cached
            use_cache=True,
            respect_robots_txt=True,
            cache_expire_after=86400 * 7  # 7 days cache
        )
        self.delay_cached = delay_cached
        self.delay_new = delay_new
        self.base_url = "https://seijiyama.jp"
        self.elections_base = f"{self.base_url}/elections"
        
        # Track progress
        self.total_requests = 0
        self.cached_requests = 0
        self.new_requests = 0
        
        # Prefecture list
        self.prefectures = [
            ("hokkaido", "ÂåóÊµ∑ÈÅì"), ("aomori", "ÈùíÊ£ÆÁúå"), ("iwate", "Â≤©ÊâãÁúå"), 
            ("miyagi", "ÂÆÆÂüéÁúå"), ("akita", "ÁßãÁî∞Áúå"), ("yamagata", "Â±±ÂΩ¢Áúå"), 
            ("fukushima", "Á¶èÂ≥∂Áúå"), ("ibaraki", "Ëå®ÂüéÁúå"), ("tochigi", "Ê†ÉÊú®Áúå"),
            ("gunma", "Áæ§È¶¨Áúå"), ("saitama", "ÂüºÁéâÁúå"), ("chiba", "ÂçÉËëâÁúå"),
            ("tokyo", "Êù±‰∫¨ÈÉΩ"), ("kanagawa", "Á•ûÂ•àÂ∑ùÁúå"), ("niigata", "Êñ∞ÊΩüÁúå"),
            ("toyama", "ÂØåÂ±±Áúå"), ("ishikawa", "Áü≥Â∑ùÁúå"), ("fukui", "Á¶è‰∫ïÁúå"),
            ("yamanashi", "Â±±Ê¢®Áúå"), ("nagano", "Èï∑ÈáéÁúå"), ("gifu", "Â≤êÈòúÁúå"),
            ("shizuoka", "ÈùôÂ≤°Áúå"), ("aichi", "ÊÑõÁü•Áúå"), ("mie", "‰∏âÈáçÁúå"),
            ("shiga", "ÊªãË≥ÄÁúå"), ("kyoto", "‰∫¨ÈÉΩÂ∫ú"), ("osaka", "Â§ßÈò™Â∫ú"),
            ("hyogo", "ÂÖµÂ∫´Áúå"), ("nara", "Â•àËâØÁúå"), ("wakayama", "ÂíåÊ≠åÂ±±Áúå"),
            ("tottori", "È≥•ÂèñÁúå"), ("shimane", "Â≥∂Ê†πÁúå"), ("okayama", "Â≤°Â±±Áúå"),
            ("hiroshima", "Â∫ÉÂ≥∂Áúå"), ("yamaguchi", "Â±±Âè£Áúå"), ("tokushima", "Âæ≥Â≥∂Áúå"),
            ("kagawa", "È¶ôÂ∑ùÁúå"), ("ehime", "ÊÑõÂ™õÁúå"), ("kochi", "È´òÁü•Áúå"),
            ("fukuoka", "Á¶èÂ≤°Áúå"), ("saga", "‰ΩêË≥ÄÁúå"), ("nagasaki", "Èï∑Â¥éÁúå"),
            ("kumamoto", "ÁÜäÊú¨Áúå"), ("oita", "Â§ßÂàÜÁúå"), ("miyazaki", "ÂÆÆÂ¥éÁúå"),
            ("kagoshima", "ÈπøÂÖêÂ≥∂Áúå"), ("okinawa", "Ê≤ñÁ∏ÑÁúå")
        ]
    
    def _make_request(self, url: str) -> tuple[str, bool]:
        """
        Make request and return (content, is_cached).
        Apply appropriate delay based on cache status.
        """
        self.total_requests += 1
        
        # Check if we have cached response
        response = self.http.get(url)
        
        if response and hasattr(response, 'from_cache') and response.from_cache:
            self.cached_requests += 1
            time.sleep(self.delay_cached)  # Fast for cached
            return response.text, True
        else:
            self.new_requests += 1
            time.sleep(self.delay_new)  # Slower for new
            return response.text if response else None, False
    
    def _get_prefecture_elections(self, romaji: str, kanji: str) -> List[str]:
        """Get all election URLs for a prefecture (uses multiple strategies)."""
        elections = []
        seen_urls = set()
        
        # Strategy 1: Main prefecture page
        pref_url = f"{self.elections_base}/pref/{romaji}/"
        content, is_cached = self._make_request(pref_url)
        
        if content:
            soup = parse_html(content)
            links = soup.find_all('a', href=re.compile(r'/area/card/'))
            
            for link in links:
                href = link.get('href')
                if href and href not in seen_urls:
                    full_url = urljoin(self.base_url, href)
                    elections.append(full_url)
                    seen_urls.add(href)
        
        # Strategy 2: Year-based archives (2015-2025)
        current_year = datetime.now().year
        for year in range(2015, current_year + 1):
            year_url = f"{self.elections_base}/pref/{romaji}/?year={year}"
            content, is_cached = self._make_request(year_url)
            
            if content:
                soup = parse_html(content)
                links = soup.find_all('a', href=re.compile(r'/area/card/'))
                
                for link in links:
                    href = link.get('href')
                    if href and href not in seen_urls:
                        full_url = urljoin(self.base_url, href)
                        elections.append(full_url)
                        seen_urls.add(href)
        
        # Strategy 3: Search page
        search_url = f"{self.elections_base}/search?prefecture={kanji}"
        content, is_cached = self._make_request(search_url)
        
        if content:
            soup = parse_html(content)
            links = soup.find_all('a', href=re.compile(r'/area/card/'))
            
            for link in links:
                href = link.get('href')
                if href and href not in seen_urls:
                    full_url = urljoin(self.base_url, href)
                    elections.append(full_url)
                    seen_urls.add(href)
        
        return elections
    
    def _extract_politicians_from_election(self, url: str, prefecture: str) -> List[Dict[str, Any]]:
        """Extract politician data from an election results page."""
        content, is_cached = self._make_request(url)
        
        if not content:
            return []
        
        soup = parse_html(content)
        politicians = []
        
        # Extract election info
        election_name = ""
        title_tag = soup.find('h1') or soup.find('h2')
        if title_tag:
            election_name = title_tag.get_text(strip=True)
        
        # Extract election date
        election_date = ""
        date_pattern = r'(\d{4})Âπ¥(\d{1,2})Êúà(\d{1,2})Êó•'
        date_match = re.search(date_pattern, election_name)
        if date_match:
            year, month, day = date_match.groups()
            election_date = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # Find candidate names
        candidate_sections = soup.find_all(['div', 'tr', 'li'], class_=re.compile(r'candidate|result|name'))
        
        if not candidate_sections:
            # Fallback: Look for any text that looks like Japanese names
            text_content = soup.get_text()
            name_pattern = r'[‰∏Ä-ÈæØ]{2,4}[\s„ÄÄ]+[‰∏Ä-ÈæØ]{2,4}'
            potential_names = re.findall(name_pattern, text_content)
            
            for name in potential_names:
                name_clean = name.strip()
                if len(name_clean) >= 2 and len(name_clean) <= 10:
                    politicians.append({
                        'name': name_clean,
                        'prefecture': prefecture,
                        'election': election_name,
                        'election_date': election_date,
                        'source': 'seijiyama.jp/elections',
                        'source_url': url,
                        'scraped_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    })
        else:
            for section in candidate_sections:
                name = section.get_text(strip=True)
                
                # Clean name
                name = re.sub(r'[0-9Ôºê-Ôºô\s„ÄÄ]', '', name)
                name = re.sub(r'(ÂΩìÈÅ∏|ËêΩÈÅ∏|Á•®|ÂæóÁ•®|Ê≠≥|Ê∞è)', '', name)
                
                if len(name) >= 2 and len(name) <= 10:
                    politicians.append({
                        'name': name,
                        'prefecture': prefecture,
                        'election': election_name,
                        'election_date': election_date,
                        'source': 'seijiyama.jp/elections',
                        'source_url': url,
                        'scraped_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    })
        
        return politicians
    
    def scrape_all_elections(self, output_file: str = "politicians_from_elections.csv") -> List[Dict[str, Any]]:
        """Scrape all elections with progress saving."""
        
        print("\n" + "="*80)
        print("‚ö° FAST ELECTION SCRAPER - Optimized for Cached Data")
        print(f"   Cached requests: {self.delay_cached}s delay")
        print(f"   New requests: {self.delay_new}s delay")
        print("   Auto-save every 1000 records")
        print("="*80 + "\n")
        
        all_politicians = []
        exporter = CSVExporter("data/outputs")
        
        start_time = time.time()
        total_prefectures = len(self.prefectures)
        
        for idx, (romaji, kanji) in enumerate(self.prefectures, 1):
            print(f"\n[{idx}/{total_prefectures}] üìç {kanji} ({romaji})")
            print("-" * 60)
            
            try:
                # Get elections
                elections = self._get_prefecture_elections(romaji, kanji)
                
                if elections:
                    print(f"  ‚úì Found {len(elections)} elections")
                    
                    # Process each election
                    for election_idx, election_url in enumerate(elections, 1):
                        if election_idx % 10 == 0:
                            print(f"  Progress: {election_idx}/{len(elections)} elections", end="\r", flush=True)
                        
                        try:
                            politicians = self._extract_politicians_from_election(election_url, kanji)
                            if politicians:
                                all_politicians.extend(politicians)
                                
                                # Auto-save every 1000 records
                                if len(all_politicians) % 1000 == 0:
                                    print(f"\n  üíæ Auto-saving {len(all_politicians):,} records...")
                                    exporter.export(all_politicians, output_file)
                        
                        except Exception as e:
                            print(f"\n  ‚ö†Ô∏è  Error in election {election_idx}: {e}")
                            continue
                    
                    print(f"  ‚úì Collected {len([p for p in all_politicians if p['prefecture'] == kanji])} politicians")
                
            except KeyboardInterrupt:
                print("\n\n‚ö†Ô∏è  Interrupted by user - saving progress...")
                if all_politicians:
                    exporter.export(all_politicians, output_file)
                    print(f"üíæ Saved {len(all_politicians):,} politicians to {output_file}")
                raise
            
            except Exception as e:
                print(f"  ‚ùå Error: {e}")
                continue
        
        # Final save
        if all_politicians:
            print(f"\n{'='*80}")
            print("üíæ Saving final results...")
            exporter.export(all_politicians, output_file)
        
        # Stats
        elapsed = time.time() - start_time
        print(f"\n{'='*80}")
        print("üìä SCRAPING COMPLETE")
        print(f"{'='*80}")
        print(f"Total politicians: {len(all_politicians):,}")
        print(f"Total requests: {self.total_requests:,}")
        print(f"  - Cached: {self.cached_requests:,} ({self.cached_requests/max(self.total_requests,1)*100:.1f}%)")
        print(f"  - New: {self.new_requests:,} ({self.new_requests/max(self.total_requests,1)*100:.1f}%)")
        print(f"Time elapsed: {elapsed/60:.1f} minutes")
        print(f"Average speed: {len(all_politicians)/(elapsed/60):.0f} politicians/minute")
        print(f"Output: data/outputs/{output_file}")
        print(f"{'='*80}\n")
        
        return all_politicians


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Fast election data scraper")
    parser.add_argument(
        '--output', '-o',
        default='politicians_from_elections_FULL.csv',
        help='Output CSV filename'
    )
    parser.add_argument(
        '--delay-cached',
        type=float,
        default=0.1,
        help='Delay for cached requests (default: 0.1s)'
    )
    parser.add_argument(
        '--delay-new',
        type=float,
        default=1.0,
        help='Delay for new requests (default: 1.0s)'
    )
    
    args = parser.parse_args()
    
    # Extract just filename if full path given
    output_filename = Path(args.output).name
    
    try:
        scraper = FastElectionScraper(
            delay_cached=args.delay_cached,
            delay_new=args.delay_new
        )
        politicians = scraper.scrape_all_elections(output_filename)
        
        print(f"\n‚úÖ Success! Collected {len(politicians):,} politicians")
        print(f"üìÅ Saved to: data/outputs/{output_filename}\n")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Scraping interrupted by user")
        print("Progress has been saved. You can restart to continue.\n")
        
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
