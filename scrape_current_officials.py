"""
Current Officials Scraper - Collect CURRENTLY SERVING public officials
ç›®æ¨™: å›½ä¼šè­°å“¡ + éƒ½é“åºœçœŒè­°ä¼šè­°å“¡ + ä¸»è¦å¸‚è­°ä¼šè­°å“¡ = 5,000-10,000 officials

This scraper focuses on CURRENT officials to maximize data quality.
Source: seijiyama.jp/member/ (public official directory)
"""

import sys
from pathlib import Path
import re
from typing import List, Dict, Any
from urllib.parse import urljoin
import time

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.core.http_client import HTTPClient
from src.core.csv_exporter import CSVExporter
from src.utils.parsers import parse_html
from datetime import datetime


class CurrentOfficialsScraper:
    """
    Scraper for currently serving public officials.
    
    Targets:
    1. National Diet (å›½ä¼š) - House of Representatives + House of Councillors (~700 members)
    2. Prefectural Assemblies (éƒ½é“åºœçœŒè­°ä¼š) - All 47 prefectures (~2,500 members)
    3. Major City Councils (å¸‚è­°ä¼š) - Top 20 cities (~1,000 members)
    
    Total target: 4,000-5,000 current officials
    """
    
    def __init__(self, delay: float = 2.0):
        """Initialize scraper with respectful delay."""
        self.http = HTTPClient(default_delay=delay, use_cache=True, respect_robots_txt=True)
        self.base_url = "https://seijiyama.jp"
        self.members_base = f"{self.base_url}/member"
        
        # Prefecture list
        self.prefectures = [
            ("hokkaido", "åŒ—æµ·é“"), ("aomori", "é’æ£®çœŒ"), ("iwate", "å²©æ‰‹çœŒ"), 
            ("miyagi", "å®®åŸçœŒ"), ("akita", "ç§‹ç”°çœŒ"), ("yamagata", "å±±å½¢çœŒ"), 
            ("fukushima", "ç¦å³¶çœŒ"), ("ibaraki", "èŒ¨åŸçœŒ"), ("tochigi", "æ ƒæœ¨çœŒ"),
            ("gunma", "ç¾¤é¦¬çœŒ"), ("saitama", "åŸ¼ç‰çœŒ"), ("chiba", "åƒè‘‰çœŒ"),
            ("tokyo", "æ±äº¬éƒ½"), ("kanagawa", "ç¥å¥ˆå·çœŒ"), ("niigata", "æ–°æ½ŸçœŒ"),
            ("toyama", "å¯Œå±±çœŒ"), ("ishikawa", "çŸ³å·çœŒ"), ("fukui", "ç¦äº•çœŒ"),
            ("yamanashi", "å±±æ¢¨çœŒ"), ("nagano", "é•·é‡çœŒ"), ("gifu", "å²é˜œçœŒ"),
            ("shizuoka", "é™å²¡çœŒ"), ("aichi", "æ„›çŸ¥çœŒ"), ("mie", "ä¸‰é‡çœŒ"),
            ("shiga", "æ»‹è³€çœŒ"), ("kyoto", "äº¬éƒ½åºœ"), ("osaka", "å¤§é˜ªåºœ"),
            ("hyogo", "å…µåº«çœŒ"), ("nara", "å¥ˆè‰¯çœŒ"), ("wakayama", "å’Œæ­Œå±±çœŒ"),
            ("tottori", "é³¥å–çœŒ"), ("shimane", "å³¶æ ¹çœŒ"), ("okayama", "å²¡å±±çœŒ"),
            ("hiroshima", "åºƒå³¶çœŒ"), ("yamaguchi", "å±±å£çœŒ"), ("tokushima", "å¾³å³¶çœŒ"),
            ("kagawa", "é¦™å·çœŒ"), ("ehime", "æ„›åª›çœŒ"), ("kochi", "é«˜çŸ¥çœŒ"),
            ("fukuoka", "ç¦å²¡çœŒ"), ("saga", "ä½è³€çœŒ"), ("nagasaki", "é•·å´çœŒ"),
            ("kumamoto", "ç†Šæœ¬çœŒ"), ("oita", "å¤§åˆ†çœŒ"), ("miyazaki", "å®®å´çœŒ"),
            ("kagoshima", "é¹¿å…å³¶çœŒ"), ("okinawa", "æ²–ç¸„çœŒ")
        ]
        
        # Major cities to scrape city council members
        self.major_cities = [
            "tokyo", "yokohama", "osaka", "nagoya", "sapporo",
            "fukuoka", "kobe", "kyoto", "kawasaki", "saitama",
            "hiroshima", "sendai", "kitakyushu", "chiba", "sakai",
            "niigata", "hamamatsu", "kumamoto", "sagamihara", "shizuoka"
        ]
    
    def scrape_all_officials(self) -> List[Dict[str, Any]]:
        """
        Scrape all current officials.
        
        Returns:
            List of official records
        """
        
        print("\n" + "="*80)
        print("ğŸ‘” CURRENT OFFICIALS SCRAPER - ç¾è·å…¬å‹™å“¡ãƒ‡ãƒ¼ã‚¿åé›†")
        print("   Target: National Diet + Prefectural Assemblies + Major Cities")
        print("   Expected: 4,000-5,000 current officials")
        print("="*80 + "\n")
        
        all_officials = []
        
        # 1. Scrape National Diet members
        print("\n" + "="*80)
        print("ğŸ“ PHASE 1: NATIONAL DIET MEMBERS (å›½ä¼šè­°å“¡)")
        print("="*80)
        national_officials = self._scrape_national_diet()
        all_officials.extend(national_officials)
        print(f"âœ… Collected {len(national_officials)} national diet members\n")
        
        # 2. Scrape Prefectural Assembly members
        print("\n" + "="*80)
        print("ğŸ“ PHASE 2: PREFECTURAL ASSEMBLY MEMBERS (éƒ½é“åºœçœŒè­°ä¼šè­°å“¡)")
        print("="*80)
        prefectural_officials = self._scrape_prefectural_assemblies()
        all_officials.extend(prefectural_officials)
        print(f"âœ… Collected {len(prefectural_officials)} prefectural assembly members\n")
        
        # 3. Scrape Major City Council members
        print("\n" + "="*80)
        print("ğŸ“ PHASE 3: MAJOR CITY COUNCIL MEMBERS (ä¸»è¦å¸‚è­°ä¼šè­°å“¡)")
        print("="*80)
        city_officials = self._scrape_city_councils()
        all_officials.extend(city_officials)
        print(f"âœ… Collected {len(city_officials)} city council members\n")
        
        print("\n" + "="*80)
        print(f"âœ… SCRAPING COMPLETE! Collected {len(all_officials)} current officials")
        print("="*80 + "\n")
        
        return all_officials
    
    def _scrape_national_diet(self) -> List[Dict[str, Any]]:
        """Scrape National Diet members (House of Representatives + House of Councillors)."""
        
        officials = []
        
        # House of Representatives (è¡†è­°é™¢)
        print("\n[1/2] ğŸ›ï¸  House of Representatives (è¡†è­°é™¢)")
        print("-" * 60)
        hr_url = f"{self.members_base}/kokkai/shugiin/"
        hr_members = self._scrape_member_list(hr_url, "National", "House of Representatives")
        officials.extend(hr_members)
        print(f"  âœ“ Found {len(hr_members)} representatives")
        
        # House of Councillors (å‚è­°é™¢)
        print("\n[2/2] ğŸ›ï¸  House of Councillors (å‚è­°é™¢)")
        print("-" * 60)
        hc_url = f"{self.members_base}/kokkai/sangiin/"
        hc_members = self._scrape_member_list(hc_url, "National", "House of Councillors")
        officials.extend(hc_members)
        print(f"  âœ“ Found {len(hc_members)} councillors")
        
        return officials
    
    def _scrape_prefectural_assemblies(self) -> List[Dict[str, Any]]:
        """Scrape all prefectural assembly members."""
        
        officials = []
        total_prefectures = len(self.prefectures)
        
        for idx, (romaji, kanji) in enumerate(self.prefectures, 1):
            print(f"\n[{idx}/{total_prefectures}] ğŸ“ {kanji} ({romaji})")
            print("-" * 60)
            
            try:
                # Prefectural assembly URL
                pref_url = f"{self.members_base}/pref/{romaji}/"
                members = self._scrape_member_list(pref_url, kanji, "Prefectural Assembly")
                officials.extend(members)
                print(f"  âœ“ Found {len(members)} assembly members")
                
            except Exception as e:
                print(f"  âœ— Error: {str(e)[:50]}")
            
            time.sleep(1)  # Brief delay between prefectures
        
        return officials
    
    def _scrape_city_councils(self) -> List[Dict[str, Any]]:
        """Scrape major city council members."""
        
        officials = []
        total_cities = len(self.major_cities)
        
        for idx, city in enumerate(self.major_cities, 1):
            print(f"\n[{idx}/{total_cities}] ğŸ™ï¸  {city.title()}")
            print("-" * 60)
            
            try:
                # City council URL
                city_url = f"{self.members_base}/city/{city}/"
                members = self._scrape_member_list(city_url, city.title(), "City Council")
                officials.extend(members)
                print(f"  âœ“ Found {len(members)} council members")
                
            except Exception as e:
                print(f"  âœ— Error: {str(e)[:50]}")
            
            time.sleep(1)  # Brief delay between cities
        
        return officials
    
    def _scrape_member_list(self, url: str, jurisdiction: str, office_type: str) -> List[Dict[str, Any]]:
        """
        Scrape a list of members from a URL.
        
        Args:
            url: URL of the member list page
            jurisdiction: Prefecture/city/national
            office_type: Type of office
            
        Returns:
            List of official dictionaries
        """
        
        try:
            response = self.http.get(url)
            html_content = response.content.decode('utf-8', errors='replace')
            soup = parse_html(html_content)
            
            members = []
            
            # Look for member links
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                
                # Look for individual member profile pages
                if '/member/detail/' in href or '/person/' in href:
                    member_url = urljoin(self.base_url, href)
                    
                    # Extract member info from the link or surrounding context
                    name = link.get_text(strip=True)
                    
                    if name and len(name) >= 2 and len(name) <= 10:
                        member = {
                            'name': name,
                            'jurisdiction': jurisdiction,
                            'office_type': office_type,
                            'status': 'Current',  # These are current officials
                            'source': 'seijiyama.jp/member',
                            'source_url': member_url,
                            'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        }
                        
                        members.append(member)
            
            # Also try to extract from tables
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    for cell in cells:
                        text = cell.get_text(strip=True)
                        if self._is_likely_official_name(text):
                            member = {
                                'name': text,
                                'jurisdiction': jurisdiction,
                                'office_type': office_type,
                                'status': 'Current',
                                'source': 'seijiyama.jp/member',
                                'source_url': url,
                                'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                            }
                            members.append(member)
            
            # Deduplicate by name
            seen = set()
            unique_members = []
            for member in members:
                if member['name'] not in seen:
                    seen.add(member['name'])
                    unique_members.append(member)
            
            return unique_members
            
        except Exception as e:
            print(f"    Error scraping {url}: {e}")
            return []
    
    def _is_likely_official_name(self, text: str) -> bool:
        """Check if text looks like an official's name."""
        
        if not text or len(text) < 2 or len(text) > 10:
            return False
        
        # Must contain Japanese characters
        if not re.search(r'[\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff]', text):
            return False
        
        # Exclude common non-name patterns
        exclude = ['æ°å', 'åå‰', 'è­°å“¡', 'ä»£è¡¨', 'é¸æŒ™', 'æŠ•ç¥¨', 'çµæœ', 'ä¸€è¦§']
        for keyword in exclude:
            if keyword in text:
                return False
        
        return True


def main():
    """Main function."""
    
    import argparse
    
    parser = argparse.ArgumentParser(description='Scrape current public officials')
    parser.add_argument(
        '--delay',
        type=float,
        default=2.0,
        help='Delay between requests in seconds (default: 2.0)'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='data/outputs/current_officials.csv',
        help='Output CSV file'
    )
    
    args = parser.parse_args()
    
    # Initialize scraper
    scraper = CurrentOfficialsScraper(delay=args.delay)
    
    # Scrape officials
    officials = scraper.scrape_all_officials()
    
    if officials:
        # Save to CSV
        exporter = CSVExporter()
        # Extract just the filename if full path provided
        output_filename = Path(args.output).name if '/' in args.output or '\\' in args.output else args.output
        output_path = exporter.export(officials, output_filename)
        
        print("\n" + "="*80)
        print("ğŸ“Š FINAL STATISTICS")
        print("="*80)
        print(f"\nâœ… Total current officials: {len(officials)}")
        
        # Count by jurisdiction
        from collections import Counter
        jurisdiction_counts = Counter(o.get('jurisdiction') for o in officials)
        print(f"\nğŸ“ By jurisdiction:")
        for jurisdiction, count in jurisdiction_counts.most_common():
            print(f"  â€¢ {jurisdiction}: {count} officials")
        
        # Count by office type
        office_counts = Counter(o.get('office_type') for o in officials)
        print(f"\nğŸ›ï¸  By office type:")
        for office, count in office_counts.most_common():
            print(f"  â€¢ {office}: {count} officials")
        
        print(f"\nğŸ’¾ Saved to: {output_path}")
        print("\n" + "="*80)
        print("âœ… SCRAPING COMPLETE!")
        print("="*80 + "\n")
    else:
        print("\nâš ï¸  No officials collected")


if __name__ == "__main__":
    main()
