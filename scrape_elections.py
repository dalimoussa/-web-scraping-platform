"""
Election Data Scraper - Extract Politician Names from Election Results
é¸æŒ™çµæœã‹ã‚‰æ”¿æ²»å®¶æƒ…å ±ã‚’æŠ½å‡º

Strategy: Scrape election results pages to build politician database.
Election results are public information, so lower risk than politician profiles.
"""

import sys
from pathlib import Path
import re
from typing import List, Dict, Any, Set
from urllib.parse import urljoin
import time

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.core.http_client import HTTPClient
from src.core.csv_exporter import CSVExporter
from src.utils.parsers import parse_html
from datetime import datetime


class ElectionDataScraper:
    """
    Scraper for election results to extract politician information.
    
    This approach scrapes PUBLIC ELECTION RESULTS rather than politician profiles,
    which has lower terms-of-service risk.
    """
    
    def __init__(self, delay: float = 3.0):
        """Initialize scraper with respectful delay."""
        self.http = HTTPClient(default_delay=delay, use_cache=True, respect_robots_txt=True)
        self.base_url = "https://seijiyama.jp"
        self.elections_base = f"{self.base_url}/elections"
        
        # Prefecture list
        self.prefectures = [
            ("hokkaido", "åŒ—æµ·é“"), ("aomori", "é’æ£®çœŒ"), ("iwate", "å²©æ‰‹çœŒ"), 
            ("miyagi", "å®®åŸçœŒ"), ("akita", "ç§‹ç”°çœŒ"), ("yamagata", "å±±å½¢çœŒ"), 
            ("fukushima", "ç¦å³¶çœŒ"), ("ibaraki", "èŒ¨åŸçœŒ"), ("tochigi", "æ ƒæœ¨çœŒ"),
            ("gunma", "ç¾¤é¦¬çœŒ"), ("saitama", "åŸ¼ç‰çœŒ"), ("chiba", "åƒè‘‰çœŒ"),
            ("tokyo", "æ±äº¬éƒ½"), ("kanagawa", "ç¥å¥ˆå·çœŒ"), ("niigata", "æ–°æ½ŸçœŒ"),
            ("toyama", "å¯Œå±±çœŒ"), ("ishikawa", "çŸ³å·çœŒ"), ("fukui", "ç¦äº•çœŒ"),
            ("yamanashi", "å±±æ¢¨çœŒ"), ("nagano", "é•·é‡çœŒ"), ("gifu", "å²é˜œçœŒ"),
            ("shizuoka", "é™å²¡çœŒ"), ("aichi", "æ„›çŸ¥çœŒ"), ("mie", "ä¸‰é‡çœŒ"),
            ("shiga", "æ»‹è³€çœŒ"), ("kyoto", "äº¬éƒ½åºœ"), ("osaka", "å¤§é˜ªåºœ"),
            ("hyogo", "å…µåº«çœŒ"), ("nara", "å¥ˆè‰¯çœŒ"), ("wakayama", "å’Œæ­Œå±±çœŒ"),
            ("tottori", "é³¥å–çœŒ"), ("shimane", "å³¶æ ¹çœŒ"), ("okayama", "å²¡å±±çœŒ"),
            ("hiroshima", "åºƒå³¶çœŒ"), ("yamaguchi", "å±±å£çœŒ"), ("tokushima", "å¾³å³¶çœŒ"),
            ("kagawa", "é¦™å·çœŒ"), ("ehime", "æ„›åª›çœŒ"), ("kochi", "é«˜çŸ¥çœŒ"),
            ("fukuoka", "ç¦å²¡çœŒ"), ("saga", "ä½è³€çœŒ"), ("nagasaki", "é•·å´çœŒ"),
            ("kumamoto", "ç†Šæœ¬çœŒ"), ("oita", "å¤§åˆ†çœŒ"), ("miyazaki", "å®®å´çœŒ"),
            ("kagoshima", "é¹¿å…å³¶çœŒ"), ("okinawa", "æ²–ç¸„çœŒ")
        ]
    
    def scrape_all_elections(self, limit_per_prefecture: int = None) -> List[Dict[str, Any]]:
        """
        Scrape election data from all prefectures.
        
        Args:
            limit_per_prefecture: Maximum elections per prefecture
            
        Returns:
            List of politician records extracted from elections
        """
        
        print("\n" + "="*80)
        print("ğŸ—³ï¸  ELECTION DATA SCRAPER - é¸æŒ™ãƒ‡ãƒ¼ã‚¿åé›†")
        print("   Strategy: Extract politician names from election results")
        print("   Lower TOS risk - election results are public information")
        print("="*80 + "\n")
        
        all_politicians = []
        seen_names = set()  # Deduplicate politicians
        
        total_prefectures = len(self.prefectures)
        
        for idx, (romaji, kanji) in enumerate(self.prefectures, 1):
            print(f"\n[{idx}/{total_prefectures}] ğŸ“ {kanji} ({romaji})")
            print("-" * 60)
            
            try:
                # Get elections for this prefecture
                elections = self._get_prefecture_elections(romaji, kanji)
                
                if elections:
                    print(f"  âœ“ Found {len(elections)} elections")
                    
                    # Limit elections if specified
                    elections_to_scrape = elections[:limit_per_prefecture] if limit_per_prefecture else elections
                    
                    # Extract politicians from each election
                    for election_idx, election_url in enumerate(elections_to_scrape, 1):
                        print(f"  [{election_idx}/{len(elections_to_scrape)}] ", end="", flush=True)
                        
                        try:
                            politicians = self._extract_politicians_from_election(election_url, kanji)
                            
                            # Add new politicians (deduplicate by name)
                            new_count = 0
                            for politician in politicians:
                                name = politician.get('name')
                                if name and name not in seen_names:
                                    seen_names.add(name)
                                    all_politicians.append(politician)
                                    new_count += 1
                            
                            print(f"âœ“ {len(politicians)} candidates ({new_count} new)")
                            
                        except Exception as e:
                            print(f"âœ— Error: {str(e)[:40]}")
                        
                        # Be respectful - delay between elections
                        time.sleep(1)
                else:
                    print(f"  âš  No elections found")
                
            except Exception as e:
                print(f"  âœ— Error accessing {kanji}: {str(e)[:50]}")
            
            # Delay between prefectures
            time.sleep(2)
            
            # Show progress
            print(f"  ğŸ“Š Total politicians collected so far: {len(all_politicians)}")
        
        print("\n" + "="*80)
        print(f"âœ… SCRAPING COMPLETE! Collected {len(all_politicians)} unique politicians")
        print("="*80 + "\n")
        
        return all_politicians
    
    def _get_prefecture_elections(self, romaji: str, kanji: str) -> List[str]:
        """Get list of election URLs for a prefecture."""
        
        # Try prefecture page
        prefecture_url = f"{self.elections_base}/pref/{romaji}/"
        
        try:
            response = self.http.get(prefecture_url)
            # Use response.content.decode to avoid encoding issues
            html_content = response.content.decode('utf-8', errors='replace')
            soup = parse_html(html_content)
            
            election_links = []
            
            # Find election links
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                
                # Look for election card/detail pages
                if '/area/card/' in href:
                    full_url = urljoin(self.base_url, href)
                    if full_url not in election_links:
                        election_links.append(full_url)
            
            return election_links
            
        except Exception as e:
            print(f"    Error getting elections: {e}")
            return []
    
    def _extract_politicians_from_election(self, election_url: str, prefecture: str) -> List[Dict[str, Any]]:
        """
        Extract candidate information from an election page.
        
        Args:
            election_url: URL of the election results page
            prefecture: Prefecture name
            
        Returns:
            List of politician dictionaries
        """
        
        try:
            response = self.http.get(election_url)
            # Use response.content.decode to avoid encoding issues
            html_content = response.content.decode('utf-8', errors='replace')
            soup = parse_html(html_content)
            
            politicians = []
            
            # Extract election name
            election_name = ""
            title_tag = soup.find('h1') or soup.find('h2')
            if title_tag:
                election_name = title_tag.get_text(strip=True)
            
            # Extract election date
            election_date = self._extract_date_from_page(soup)
            
            # Look for candidate tables
            tables = soup.find_all('table')
            
            for table in tables:
                rows = table.find_all('tr')
                
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    
                    if len(cells) < 1:
                        continue
                    
                    # Extract candidate name
                    for cell in cells:
                        text = cell.get_text(strip=True)
                        
                        # Look for Japanese names (2-5 characters with kanji/hiragana)
                        if self._is_likely_candidate_name(text):
                            politician = {
                                'name': text,
                                'prefecture': prefecture,
                                'election': election_name,
                                'election_date': election_date,
                                'source': 'seijiyama.jp/elections',
                                'source_url': election_url,
                                'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                            }
                            
                            # Try to extract party (often in nearby cell)
                            party = self._extract_party_from_row(row)
                            if party:
                                politician['party'] = party
                            
                            # Try to extract status (å½“é¸/elected, è½é¸/defeated)
                            status = self._extract_election_status(row)
                            if status:
                                politician['status'] = status
                            
                            # Try to extract vote count
                            votes = self._extract_vote_count(row)
                            if votes:
                                politician['votes'] = votes
                            
                            politicians.append(politician)
            
            # Also look for candidate lists in div/li elements
            candidate_divs = soup.find_all(['div', 'li'], class_=re.compile(r'candidate|person|member', re.I))
            
            for div in candidate_divs:
                name = self._extract_name_from_element(div)
                if name and self._is_likely_candidate_name(name):
                    politician = {
                        'name': name,
                        'prefecture': prefecture,
                        'election': election_name,
                        'election_date': election_date,
                        'source': 'seijiyama.jp/elections',
                        'source_url': election_url,
                        'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                    
                    # Extract additional info
                    party = self._extract_party_from_element(div)
                    if party:
                        politician['party'] = party
                    
                    politicians.append(politician)
            
            return politicians
            
        except Exception as e:
            raise Exception(f"Failed to extract from {election_url}: {e}")
    
    def _is_likely_candidate_name(self, text: str) -> bool:
        """Check if text looks like a candidate name."""
        
        if not text or len(text) < 2 or len(text) > 8:
            return False
        
        # Must contain kanji or hiragana
        if not re.search(r'[\u4e00-\u9fff\u3040-\u309f]', text):
            return False
        
        # Exclude common non-name text
        exclude_patterns = [
            r'é¸æŒ™', r'æŠ•ç¥¨', r'çµæœ', r'å€™è£œ', r'å½“é¸', r'è½é¸',
            r'å¸‚é•·', r'çŸ¥äº‹', r'è­°å“¡', r'æœˆ', r'æ—¥', r'å¹´',
            r'ç¥¨', r'å¾—ç¥¨', r'é–‹ç¥¨', r'ç¢ºå®š', r'é€Ÿå ±',
            r'ç„¡æ‰€å±', r'æ”¿å…š', r'æ”¯æŒ', r'æ¨è–¦'
        ]
        
        for pattern in exclude_patterns:
            if re.search(pattern, text):
                return False
        
        return True
    
    def _extract_name_from_element(self, element) -> str:
        """Extract candidate name from HTML element."""
        
        # Try name-specific tags first
        name_tag = element.find(class_=re.compile(r'name', re.I))
        if name_tag:
            return name_tag.get_text(strip=True)
        
        # Otherwise return element text
        return element.get_text(strip=True).split()[0] if element.get_text(strip=True) else ""
    
    def _extract_party_from_row(self, row) -> str:
        """Extract party affiliation from table row."""
        
        cells = row.find_all(['td', 'th'])
        
        for cell in cells:
            text = cell.get_text(strip=True)
            
            # Check for common party names
            parties = [
                'è‡ªæ°‘', 'è‡ªç”±æ°‘ä¸»å…š', 'ç«‹æ†²', 'ç«‹æ†²æ°‘ä¸»å…š', 'å…¬æ˜', 'å…¬æ˜å…š',
                'å…±ç”£', 'å…±ç”£å…š', 'å›½æ°‘æ°‘ä¸»', 'ç¶­æ–°', 'æ—¥æœ¬ç¶­æ–°ã®ä¼š',
                'ç¤¾æ°‘', 'ç¤¾ä¼šæ°‘ä¸»å…š', 'ã‚Œã„ã‚', 'ã‚Œã„ã‚æ–°é¸çµ„',
                'NHKå…š', 'NHKã‹ã‚‰å›½æ°‘ã‚’å®ˆã‚‹å…š', 'ç„¡æ‰€å±'
            ]
            
            for party in parties:
                if party in text:
                    return text
        
        return ""
    
    def _extract_party_from_element(self, element) -> str:
        """Extract party from HTML element."""
        
        party_tag = element.find(class_=re.compile(r'party|affiliation|æ‰€å±', re.I))
        if party_tag:
            return party_tag.get_text(strip=True)
        
        # Check element text for party keywords
        text = element.get_text()
        parties = [
            'è‡ªæ°‘', 'ç«‹æ†²', 'å…¬æ˜', 'å…±ç”£', 'å›½æ°‘æ°‘ä¸»', 'ç¶­æ–°',
            'ç¤¾æ°‘', 'ã‚Œã„ã‚', 'NHKå…š', 'ç„¡æ‰€å±'
        ]
        
        for party in parties:
            if party in text:
                return party
        
        return ""
    
    def _extract_election_status(self, row) -> str:
        """Extract election result status (å½“é¸/è½é¸)."""
        
        text = row.get_text()
        
        if 'å½“é¸' in text or 'å½“' in text:
            return 'å½“é¸'
        elif 'è½é¸' in text or 'è½' in text:
            return 'è½é¸'
        
        return ""
    
    def _extract_vote_count(self, row) -> str:
        """Extract vote count from row."""
        
        cells = row.find_all(['td', 'th'])
        
        for cell in cells:
            text = cell.get_text(strip=True)
            
            # Look for numbers that might be vote counts
            if re.match(r'^\d{1,10}$', text):
                return text
        
        return ""
    
    def _extract_date_from_page(self, soup) -> str:
        """Extract election date from page."""
        
        # Look for date patterns in text
        text = soup.get_text()
        
        # Pattern: YYYYå¹´MMæœˆDDæ—¥
        date_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', text)
        if date_match:
            year, month, day = date_match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # Pattern: YYYY/MM/DD
        date_match = re.search(r'(\d{4})/(\d{1,2})/(\d{1,2})', text)
        if date_match:
            year, month, day = date_match.groups()
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        return ""


def main():
    """Main function."""
    
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Scrape politician data from election results'
    )
    parser.add_argument(
        '--prefectures', '-p',
        nargs='+',
        help='Specific prefectures to scrape (romaji names)'
    )
    parser.add_argument(
        '--limit-per-prefecture', '-l',
        type=int,
        help='Maximum elections per prefecture'
    )
    parser.add_argument(
        '--delay', '-d',
        type=float,
        default=3.0,
        help='Delay between requests in seconds (default: 3.0)'
    )
    parser.add_argument(
        '--output', '-o',
        default='election_politicians.csv',
        help='Output CSV file'
    )
    
    args = parser.parse_args()
    
    # Initialize scraper
    scraper = ElectionDataScraper(delay=args.delay)
    
    # Filter prefectures if specified
    if args.prefectures:
        scraper.prefectures = [
            (romaji, kanji) for romaji, kanji in scraper.prefectures
            if romaji in args.prefectures
        ]
        print(f"\nğŸ“ Limiting to {len(scraper.prefectures)} prefectures: {', '.join(args.prefectures)}\n")
    
    # Scrape elections
    politicians = scraper.scrape_all_elections(
        limit_per_prefecture=args.limit_per_prefecture
    )
    
    if politicians:
        # Save to CSV
        exporter = CSVExporter()
        output_path = exporter.export(politicians, args.output)
        
        print("\n" + "="*80)
        print("ğŸ“Š FINAL STATISTICS")
        print("="*80)
        print(f"\nâœ… Total unique politicians: {len(politicians)}")
        
        # Count by prefecture
        from collections import Counter
        prefecture_counts = Counter(p.get('prefecture') for p in politicians)
        print(f"\nğŸ“ Top prefectures:")
        for pref, count in prefecture_counts.most_common(10):
            print(f"  â€¢ {pref}: {count} politicians")
        
        # Count by party
        party_counts = Counter(p.get('party', 'ä¸æ˜') for p in politicians if p.get('party'))
        if party_counts:
            print(f"\nğŸ›ï¸  Top parties:")
            for party, count in party_counts.most_common(10):
                print(f"  â€¢ {party}: {count} politicians")
        
        # Data quality
        with_party = sum(1 for p in politicians if p.get('party'))
        with_status = sum(1 for p in politicians if p.get('status'))
        with_date = sum(1 for p in politicians if p.get('election_date'))
        
        print(f"\nğŸ“ˆ Data quality:")
        print(f"  â€¢ With party: {with_party} ({with_party/len(politicians)*100:.1f}%)")
        print(f"  â€¢ With election result: {with_status} ({with_status/len(politicians)*100:.1f}%)")
        print(f"  â€¢ With election date: {with_date} ({with_date/len(politicians)*100:.1f}%)")
        
        print(f"\nğŸ“ Output: {output_path}\n")
        
        # Show sample
        print("ğŸ“‹ Sample politicians:")
        for i, p in enumerate(politicians[:10], 1):
            print(f"\n{i}. {p.get('name', 'N/A')}")
            if p.get('party'):
                print(f"   Party: {p.get('party')}")
            if p.get('prefecture'):
                print(f"   Prefecture: {p.get('prefecture')}")
            if p.get('election'):
                print(f"   Election: {p.get('election')}")
            if p.get('status'):
                print(f"   Result: {p.get('status')}")
        
        print("\n" + "="*80)
        print("âœ… ELECTION DATA SCRAPING COMPLETE!")
        print("="*80 + "\n")
        
    else:
        print("\nâŒ No politician data extracted.")
        print("\nğŸ’¡ Troubleshooting:")
        print("  1. Check if seijiyama.jp/elections/ is accessible")
        print("  2. Review HTML structure of election pages")
        print("  3. Try with specific prefectures: --prefectures tokyo osaka")
        print("  4. Increase delay: --delay 5.0\n")


if __name__ == "__main__":
    main()
